---
editor_options:
  chunk_output_type: inline
title: "Routinely Randomize Potential Sources of Measurement Reactivity to Estimate and Adjust for Biases in Subjective Reports"
output: 
  pdf_document: 
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
---


```{r}
options(stringsAsFactors = FALSE)
#' show two significant digits tops
options(digits = 2)
#' tend not to show scientific notation, because we're just psychologists
options(scipen = 7)
#' make output a bit wider
options(width = 110)
#' set a seed to make analyses depending on random number generation reproducible
set.seed(1710) # if you use your significant other's birthday make sure you stay together for the sake of reproducibility


#' ## Load packages
#' generate the site
library(rmarkdown)
#' set options for chunks
library(knitr)
#' my formr utility package to generate e.g. the bibliography
library(formr)
#' pretty-printed output
library(pander)
#' tidyverse date times
library(lubridate)
#' tidyverse strings
library(stringr)
#' extractor functions for models
library(broom)
#' grammar of graphics plots
library(ggplot2)
#' svg graphs
# library(svglite);
library(feather)
library(ggthemes)
library(codebook)
library(kableExtra)
library(Cairo)
library(paletteer)

#' tidyverse: has a lot of naming conflicts, so always load last
library(tidyverse)
opts_chunk$set(warning = F, message = F, error = TRUE, fig.width = 13, fig.height = 10)
library(broom.mixed)
library(tidylog)
options(width = 4000)
theme_set(theme_classic() + theme_pander(base_size = 18))
```


```{r}
load("../routine_and_sex/cleaned_selected.rdata")
diary <- rio::import("../routine_and_sex/data/diary_item_data.feather")
s3_daily_id <- rio::import("../routine_and_sex/data/s3_daily_itemdisplay_processed.feather")
s3_daily_id_answered <-  rio::import("../routine_and_sex/data/s3_daily_id_answered.feather")
first_page <-  rio::import("../routine_and_sex/data/first_page.feather")
time_items <-  rio::import("../routine_and_sex/data/time_items.feather")
desire_items <-  rio::import("../routine_and_sex/data/desire_items.feather")
retention_items <- rio::import("../routine_and_sex/data/retention_items.feather")
```

## Additional descriptive statistics

```{r fig.cap="Weekday distribution on diary starting day. We see that women were more likely to start the study on a week day."}
ggplot(diary %>% drop_na(created_diary) %>% filter(day_number == 0), aes(weekday)) + 
  geom_bar() +
  ggrepel::geom_label_repel(aes(x = weekday, y = stat(count), label = stat(count)), 
                            stat = "count", force = 0)
```

```{r fig.cap="Distributions of items", fig.height=5, fig.width=5}
first_page %>% ggplot(aes(answer)) + 
  geom_bar() + 
  facet_wrap(~ label_english, ncol = 2, scales = "free_y") +
  scale_x_continuous("Response", breaks = 0:4, labels = c("[0] less\nthan\nusual", 1, 2, 3, "[4] more\nthan\nusual")) +
  ggrepel::geom_label_repel(aes(x = answer, y = stat(count), label = stat(count)), 
                            stat = "count", force = 0, size = 2)
```

```{r fig.cap="A quick verification that fluctuation in whether a question is asked is as random as intended over time. The Y axes include the global mean ± 1 global standard deviation for each item.", fig.width=5,fig.height=6}
first_page %>% 
  group_by(day_number) %>% 
  mutate(n_days = n_distinct(session)) %>% 
  group_by(label_english, day_number) %>% 
  summarise(n = n()/first(n_days)) %>% 
ggplot(., aes(day_number, n)) + 
  geom_line() +
  scale_y_continuous("Question asked") +
  scale_x_continuous("Day number") +
  facet_wrap(~ label_english, ncol = 2)
```

```{r fig.cap="No strong time trends in the responses. Y axis range is overall mean ±1SD.", fig.width=5,fig.height=6}
first_page %>% 
  ggplot(., aes(day_number, answer)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_hline(aes(yintercept = item_mean), linetype = 'dashed') + 
  geom_pointrange(position = position_dodge(width = 0.2), stat='summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.2), stat='summary', fun.data = 'mean_se') +
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, ncol = 2, scales = "free_y") +
  scale_x_continuous("Day number") +
  ggtitle("Responses over time")
```


## Initial elevation bias

```{r fig.cap="Figure 2, including combinations with fewer than 20 observations."}
first_page %>% 
  filter(day_number < 7) %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(answer, na.rm = TRUE)) %>%
  # group_by(item_name, day_number, first_day_of_item_factor) %>%
  # filter(n_nonmissing(answer) > 20) %>%
  ggplot(., aes(day_number, answer, colour = first_day_of_item_factor)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_hline(aes(yintercept = group_mean, group = label), color = "gray", linetype = 'dashed') +
  geom_pointrange(position = position_dodge(width = 0.2), stat = 'summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat = 'summary', fun.data = 'mean_se') + 
  scale_color_colorblind("First day the\nitem was shown") +
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, scales = "free_y") + 
  scale_x_continuous("Day number", breaks = 0:10)
```


```{r fig.cap="An alternative, more focused visualisation of initial elevation bias. Are responses elevated when shown for the first time, even if it is not the first day of the diary? The dashed line shows the overall mean for the item. Standard errors cannot be seen, because they are so narrow (and ignore the multilevel structure of the data."}
first_page %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(answer, na.rm = TRUE),
         day = if_else(first_day_of_item_shown, if_else(day_number == 0, 
                       "first item, \nfirst day", "first item, \nlater day"), "later day")) %>%
  group_by(item_name, day) %>% 
  filter(n_nonmissing(answer) > 20) %>% 
  ggplot(., aes(day, answer)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_hline(aes(yintercept = group_mean, group = label), color = "gray", linetype = 'dashed') +
  geom_pointrange(position = position_dodge(width = 0.2), stat = 'summary', fun.data = 'mean_se') + 
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, scales = "free_y")
```



```{r fig.cap="A response bias to a Likert scale may not only affect the mean response, but also the dispersion or the propensity to choose the middle or extreme categories. Such biases would balance out and not show up in the mean response. We therefore compute the relative frequency of certain responses for first days and later days. Days on which the item is first shown have very similar response distributions as later days."}
first_page %>% 
  filter(day_number < 11) %>% 
  mutate(day = if_else(first_day_of_item_shown, if_else(day_number == 0, 
                       "first item, \nfirst day", "first item, \nlater day"), "later day")) %>%
  group_by(label_english, item_name, day) %>% 
  mutate(group_n = n_nonmissing(answer)) %>% 
  group_by(label_english, item_name, day, answer) %>% 
  summarise(rel_freq = n_nonmissing(answer)/first(group_n)) %>% 
  ggplot(., aes(answer, y = rel_freq, colour = day)) + 
  geom_line(position = position_dodge(width = 0.2)) + 
  scale_y_continuous("Relative frequency") +
  scale_x_continuous("Response", breaks = 0:4, labels = c("[0] less\nthan\nusual", 1, 2, 3, "[4] more\nthan\nusual")) +
  facet_wrap(~ label_english, scales = "free_y")
```


```{r fig.cap="We can additionally examine whether responses slow down when items are first shown. We only examine the response time relative to the answer to the previous item here. This means the first item is excluded from consideration. We do this, because responses relative to the time the page loaded are strongly biased upwards through participants who clicked the link and did something else until the page loaded, or participants who first familiarise themselves with all items. Responses to the first item take almost 8000ms, much longer than responses to later items. The line shows the 10% trimmed means, the points show means plus standard errors. We excluded responses that were made out of order (negative response times relative to the previous item), and responses that took longer than 30 seconds."}
first_page %>% 
  filter(day_number < 11) %>% 
  filter(response_time_since_previous < 1*30*1000, response_time_since_previous > 0) %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(response_time_since_previous, na.rm = TRUE),
         day = if_else(first_day_of_item_shown, if_else(day_number == 0, 
                       "first item, \nfirst day", "first item, \nlater day"), "later day")) %>%
  group_by(item_name, day) %>% 
  filter(n_nonmissing(response_time_since_previous) > 20) %>% 
  ggplot(., aes(day, response_time_since_previous)) + 
  geom_hline(aes(yintercept = group_mean, group = label), color = "gray", linetype = 'dashed') +
  geom_pointrange(position = position_dodge(width = 0.2), stat = 'summary', fun.data = 'mean_se') + 
  scale_y_continuous("Response time relative to previous item (in ms)") +
  facet_wrap(~ label_english, scales = "free_y")
```


```{r fig.cap="We can also examine response times as in Figure 2, but switching to the time series view somewhat clutters the display. Limited to combinations with at least 20 responses."}
first_page %>% 
  filter(day_number < 11, response_time_since_previous < 30*1000, response_time_since_previous > 0) %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(answer, na.rm = TRUE)) %>% 
  group_by(item_name, day_number, first_day_of_item_factor) %>% 
  filter(n_nonmissing(answer) > 20) %>% 
  ggplot(., aes(day_number, response_time_since_previous, colour = first_day_of_item_factor)) + 
  # geom_pointrange(alpha = 0.3, position = position_dodge(width = 0.2), stat ='summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat ='summary', fun.y = function(x) { mean(x, na.rm =T, trim = 0.1) }) +
  scale_color_colorblind("First day the\nitem was shown") +
  scale_x_continuous("Day number", breaks = 0:10) +
  scale_y_continuous("Response time (in ms) since previous item") +
  facet_wrap(~ label_english)
```


### Time series by first day item shown (complete 1st week)
Here, only with those who didn't miss a day in the first week (ruling out selective dropout as an explanation). 
Patterns seem unchanged.

```{r}
first_page %>% 
  filter(day_number < 7, didntmissfirstweek == TRUE) %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(answer, na.rm = TRUE)) %>% 
  group_by(item_name, day_number, first_day_of_item_factor) %>% 
  filter(n_nonmissing(answer) > 20) %>% 
  ggplot(., aes(day_number, answer, colour = first_day_of_item_factor)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_hline(aes(yintercept = group_mean, group = label), color = "gray", linetype = 'dashed') +
  geom_pointrange(position = position_dodge(width = 0.2), stat = 'summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat = 'summary', fun.data = 'mean_se') + 
  scale_color_colorblind("First day the\nitem was shown") +
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, scales = "free_y") + 
  scale_x_continuous("Day number", breaks = 0:10)
```

```{r}
first_page %>% 
  filter(day_number < 11, didntmissfirstweek == TRUE) %>% 
  mutate(day = if_else(first_day_of_item_shown, if_else(day_number == 0, 
                       "first item, \nfirst day", "first item, \nlater day"), "later day")) %>%
  group_by(label_english, item_name, day) %>% 
  mutate(group_n = n_nonmissing(answer)) %>% 
  group_by(label_english, item_name, day, answer) %>% 
  summarise(rel_freq = n_nonmissing(answer)/first(group_n)) %>% 
  ggplot(., aes(answer, y = rel_freq, colour = day)) + 
  geom_line(position = position_dodge(width = 0.2)) + 
  scale_y_continuous("Relative frequency") +
  facet_wrap(~ label_english, scales = "free_y")
```


As above, we can not only examine the mean but also the relative frequencies of 
each response.

```{r}
first_page %>% 
  group_by(label_english, item_name, display_order) %>% 
  mutate(group_n = n_nonmissing(answer)) %>% 
  group_by(label_english, item_name, display_order, answer) %>% 
  summarise(rel_freq = n_nonmissing(answer)/first(group_n)) %>% 
  ggplot(., aes(answer, y = rel_freq, colour = display_order, group = display_order)) + 
  geom_line(position = position_dodge(width = 0.2)) + 
  scale_color_continuous("Item order") +
  scale_y_continuous("Relative frequency") +
  scale_x_continuous("Response", breaks = 0:4, labels = c("[0] less\nthan\nusual", 1, 2, 3, "[4] more\nthan\nusual")) +
  facet_wrap(~ label_english, scales = "free_y")
```


Again, we can also examine the response time to each item according to item order.


```{r}
first_page %>% filter(response_time_since_previous < 1*30*1000, response_time_since_previous > 0, display_order > 1) %>% 
  ggplot(., aes(display_order, response_time_since_previous)) + 
  geom_pointrange(alpha = 0.3, position = position_dodge(width = 0.2), stat ='summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat ='summary', fun.y = function(x) { mean(x, na.rm =T, trim = 0.1) }) +
  scale_y_continuous("Response time (in ms) since previous item") +
  facet_wrap(~ label_english, scales = 'free_y')
```


## Number of items shown
As mentioned above, the number of items shown is also a randomised variable.
It is in turn confounded with item order, because items shown on a page with more items
are likely to have a later item order. Most importantly, when only one item is shown, item order
is also one.

```{r}
first_page %>% 
  ggplot(., aes(number_of_items_shown, answer)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_pointrange(position = position_dodge(width = 0.2), stat ='summary', fun.data = 'mean_se') + 
  # geom_line(position = position_dodge(width = 0.4), stat ='summary', fun.data = 'median_hilow') + 
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, scales = 'free_y')
```


Again, we can also examine the response time to each item according to item order.

```{r}
first_page %>% filter(response_time_since_previous < 1*30*1000, response_time_since_previous > 0, display_order > 1) %>% 
  ggplot(., aes(number_of_items_shown, response_time_since_previous)) + 
  geom_pointrange(alpha = 0.3, position = position_dodge(width = 0.2), stat ='summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat ='summary', fun.y = function(x) { mean(x, na.rm =T, trim = 0.1) }) +
  scale_y_continuous("Response time (in ms) since previous item") +
  facet_wrap(~ label_english, scales = 'free_y')
```



## Last item
Given that item order is randomised, it seems fruitful to examine whether the
previous question biases the next. This would be a potential mechanism for item
order effects. Some differences are apparent, though minute,


```{r}
first_page %>% 
  mutate(last_item = relevel(factor(last_item), "none")) %>% 
  ggplot(., aes(last_item, answer)) +
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_pointrange(position = position_dodge(width = 0.2), stat ='summary', fun.data = 'mean_se') + 
  # geom_line(position = position_dodge(width = 0.4), stat ='summary', fun.data = 'median_hilow') + 
  scale_y_continuous("Response") +
  scale_x_discrete("Preceding item") +
  coord_flip() +
  facet_wrap(~ label_english, scales = 'free_x')

ggsave(width = 15, height = 8, filename = "Figure4.png")
```

It is not possible to infer whether the item content would bias the next response
(i.e., a reminder of stress truly lowers mood) or whether these are the function
of participants minimising "mousework" (i.e., after responding 4 to one item,
it is slightly less effort to answer 4 for the next item too than to choose
a different response, and certain items elicit higher mean responses).

We can exclude people who gave the same response to items as a robustness check.
Of course, giving the same response to all items is not that unlikely when only two
items were asked, and it is entirely possible for straightline response to be 
legitimate (even if they exceed the nominal probability expected if responses were
independent, they may be more frequent on very quotidian days in a way that is 
difficult to model). Still, as a robustness check it will do.

```{r}
first_page %>% 
  group_by(session, number_of_items_shown, unit_session_id) %>% 
  summarise(straightliners = sd(answer, na.rm = TRUE) == 0) %>% 
  group_by(number_of_items_shown) %>% 
  summarise(mean = mean(straightliners, na.rm = TRUE)) %>% 
  mutate(expected_if_independent = 0.36^number_of_items_shown +
                                   0.23^number_of_items_shown +
                                   0.21^number_of_items_shown +
                                   0.12^number_of_items_shown +
                                   0.07^number_of_items_shown) %>% 
  knitr::kable()
  
first_page %>% 
  mutate(last_item = relevel(factor(last_item), "none")) %>% 
  group_by(session, unit_session_id) %>% 
  filter(sd(answer, na.rm = TRUE) > 0) %>% 
  ggplot(., aes(last_item, answer)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_pointrange(position = position_dodge(width = 0.2), stat ='summary', fun.data = 'mean_se') + 
  # geom_line(position = position_dodge(width = 0.4), stat ='summary', fun.data = 'median_hilow') + 
  scale_y_continuous("Response") +
  coord_flip() +
  facet_wrap(~ label_english, scales = 'free_x')
```


As above, we can not only examine the mean but also the relative frequencies of 
each response.

```{r}
first_page %>% 
  mutate(last_item = relevel(factor(last_item), "none")) %>% 
  group_by(label_english, item_name, last_item) %>% 
  mutate(group_n = n_nonmissing(answer)) %>% 
  group_by(label_english, item_name, last_item, answer) %>% 
  summarise(rel_freq = n_nonmissing(answer)/first(group_n)) %>% 
  ggplot(., aes(answer, y = rel_freq, colour = last_item)) + 
  geom_line(position = position_dodge(width = 0.2)) + 
  scale_color_discrete("Last item") +
  scale_y_continuous("Relative frequency") +
  scale_x_continuous("Response", breaks = 0:4, labels = c("[0] less\nthan\nusual", 1, 2, 3, "[4] more\nthan\nusual")) +
  facet_wrap(~ label_english, scales = "free_y")
```


## Multilevel analysis 
We have investigated each randomised variable in turn, but we also noted that
item order and number of items shown are confounded with one another. In addition,
the first day an item is shown is likely to be an earlier day of the diary, where we might
expect to see selection bias. Further, we varied the instructions in the diary depending
on how long ago the last diary was answered. We instructed participants to refer to the time period since their last diary entry, if that had happened within the last 24 hours, or to the last 24 hours, if the last diary entry was longer ago (and if it was their first diary entry).

In addition, a more generalised understanding of the initial elevation bias might 
lead us to believe that responses are continuously elevated more, 
the fewer times an item has been shown. 

A natural way to disentangle these confounds is to simultaneously enter them into a 
regression. We fit multilevel regression per item in lme4.


```{r}
library(lme4)
library(lmerTest)
library(purrr)
library(DT)
show_dt <- . %>% 
  datatable(rownames = FALSE, filter = "top", options = list( pageLength = 200)) %>% 
  formatRound(c("estimate", "std.error", "statistic", "conf.low", "conf.high"), digits = 2) %>% 
  formatRound(c("df"), digits = 0) %>%
  formatSignif(c("p.value"), digits = 5)
```

### Initial elevation bias
Here, we are adjusting for day number (0 to 7+, larger numbers are binned because covariates can no longer be isolated) and the time period referred to. We also enter a random effect for which day the item was first shown, and the participant.

```{r}
initial_elevation_bias <- first_page %>% 
  split(.$item_name) %>%
  map(~ lmer(answer ~ day_number_factor + first_day_of_item_shown + refer_time_period + (1 | session), data = .)) %>% 
  map(~ tidy(., conf.int = TRUE)) %>% 
  bind_rows(.id = "response") %>% 
  filter(term == "first_day_of_item_shownTRUE")
```

```{r layout='l-body-outset'}
initial_elevation_bias %>% 
  show_dt()
```

```{r layout='l-body-outset'}
initial_elevation_bias %>% 
  ggplot(aes(x = response, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_pointrange() +
  coord_flip()
```

### Item order and number

```{r}
item_order_bias <- first_page %>% 
  split(.$item_name) %>%
  map(~ lmer(answer ~ day_number_factor + display_order  + number_of_items_shown + refer_time_period + (1 | session), data = .)) %>% 
  map(~ tidy(., conf.int = TRUE)) %>% 
  bind_rows(.id = "response") %>% 
  filter(term %in% c("number_of_items_shown", "display_order"))
```

```{r layout='l-body-outset'}
item_order_bias %>% 
  show_dt()
```

```{r layout='l-body-outset'}
item_order_bias %>% 
  filter(term == "display_order") %>% 
  ggplot(aes(x = response, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_pointrange() +
  coord_flip()
```

### All randomised exposures
We have three randomised variables:

- display order
- number of items shown
- the times the item was seen already (reference category: first day)
- last item

We adjust for day number (0 to 7+), the time period referred to (confounded with how often people have responded so far), random effects for the participant

```{r fig.width=10,fig.height=8}
predictors <- first_page %>% 
  mutate(item_order = factor(display_order),
         times_item_shown = times_item_answered_factor,
         items_shown = factor(number_of_items_shown),
         last_item = relevel(factor(last_item), "none"),
         dnr = day_number,
         day_number = day_number_factor
         )

complex_mods <-  predictors %>% 
  split(.$item_name) %>%
  map(~ lmer(answer ~ items_shown + item_order + times_item_shown + day_number + refer_time_period +  (1 | session), data = .))


all_biases <- complex_mods %>% 
  map(~ tidy(., conf.int = TRUE, conf.level = 0.99)) %>% 
  bind_rows(.id = "response") 


all_biases %>% 
  filter(term != "refer_time_periodlast entry", 
         str_sub(term, 1, 10) != "day_number", term != "(Intercept)", !is.na(conf.high)) %>% 
  ggplot(aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  facet_wrap(~ response) +
  geom_pointrange() +
  coord_flip()
```

Is there evidence for an additional bias of the last item identity? We test this separately because item order 1 and last item "none" are identical, and estimates become less precise because of similar, less severe multicollinearity.

```{r}
predictors %>% 
  split(.$item_name) %>%
  map(~ as.data.frame(anova(
    lmer(answer ~ items_shown + item_order + times_item_shown + day_number + refer_time_period +  (1 | session), data = ., REML = FALSE),
    lmer(answer ~ last_item + items_shown + item_order + times_item_shown + day_number + refer_time_period +  (1 | session), data = .), REML = FALSE)
    )) %>% 
  bind_rows(.id = "response") %>% 
  knitr::kable()
```


How much do our variables change when we residualise for all of these biases?

```{R}
predictors %>% 
  split(.$item_name) %>%
  map(~ cor(.$answer, residuals(lm(answer ~ items_shown + last_item + times_item_shown + day_number + refer_time_period, data = .)))) %>% 
  bind_rows() %>% 
  gather(variable, cor) %>% 
  arrange(cor) %>% 
  knitr::kable()
```

Not much at all.

<details><summary>Allow individual differences in biases</summary>

Here, we additionally residualise the item response for inter-individual person-level intercepts and then see whether further residualising for the average biases and person-level varying biases makes a large difference. It does not. In this model, only display order, number of items shown, and times item answered are adjusted for continuously, or the number of random effects would exceed the number of rows.

```{r}
predictors %>% 
  split(.$item_name) %>%
  map(~ cor(residuals(lmer(answer ~ (1 | session), .)), fitted(lmer(answer ~ scale(display_order) + scale(number_of_items_shown) + scale(times_item_answered) + (1 + scale(display_order) + scale(number_of_items_shown) + scale(times_item_answered) | session), data = .)))^2) %>% 
  bind_rows() %>% 
  gather(variable, cor) %>% 
  arrange(cor) %>% 
  knitr::kable()

```

</details>


#### Regression table
```{r layout='l-body-outset'}
all_biases %>% 
  show_dt()
```

### Continuous predictors
```{r}
predictors <- first_page %>% 
  mutate(item_order = display_order,
         times_item_shown = times_item_answered,
         items_shown = number_of_items_shown
         )
complex_mods <-  predictors %>% 
  split(.$item_name) %>%
  map(~ lmer(answer ~ items_shown + item_order + times_item_shown + day_number + refer_time_period +  (1 | session), data = .))


all_biases <- complex_mods %>% 
  map(~ tidy(., conf.int = TRUE, conf.level = 0.99)) %>% 
  bind_rows(.id = "response") 


all_biases %>% 
  filter(term != "refer_time_periodlast entry", 
         str_sub(term, 1, 10) != "day_number", term != "(Intercept)", !is.na(conf.high)) %>% 
  ggplot(aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  facet_wrap(~ response) +
  geom_pointrange() +
  coord_flip()
```

How much do our variables change when we residualise for all of these biases?

```{R}
predictors %>% 
  split(.$item_name) %>%
  map(~ cor(.$answer, residuals(lm(answer ~ items_shown + last_item + times_item_shown + day_number + refer_time_period, data = .)))) %>% 
  bind_rows() %>% 
  gather(variable, cor) %>% 
  arrange(cor) %>% 
  knitr::kable()
```

#### Regression table
```{r layout='l-body-outset'}
all_biases %>% 
  show_dt()
```


### Regression table
```{r layout='l-body-outset'}
initial_elevation_bias_rt %>% 
  show_dt()
```


## Other items

We focused on the most general items on the first page of our study.
However, the diary also contained randomised (in order and odds of appearing) items about sexual desire, time use, and partner jealousy, among others. We show that
the overall results (that residualising for estimated biases has negligible effects) holds here too, even though the partner jealousy items were asked on a response scale from "not at all" to "very much", the desire items on a scale of "very inaccurate" to "very accurate", and the time use items on the same "less than usual" to "more than usual" scale as the items on the first page.

### Time items


On  a "less than usual" to "more than usual" response scale.



```{r}
time_items %>% group_by(item_name, label_english, choices) %>% 
  summarise(women = n_distinct(session),
            days = n_distinct(unit_session_id)) %>% 
  knitr::kable()
```

```{r}
time_items %>% 
  filter(day_number < 7) %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(answer, na.rm = TRUE)) %>% 
  group_by(item_name, day_number, first_day_of_item_factor) %>% 
  filter(n_nonmissing(answer) > 20) %>% 
  ggplot(., aes(day_number, answer, colour = first_day_of_item_factor)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_hline(aes(yintercept = group_mean, group = label), color = "gray", linetype = 'dashed') +
  geom_pointrange(position = position_dodge(width = 0.2), stat = 'summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat = 'summary', fun.data = 'mean_se') + 
  scale_color_colorblind("First day the\nitem was shown") +
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, scales = "free_y", nrow = 3) + 
  scale_x_continuous("Day number", breaks = 0:10)
```


```{r fig.width=10,fig.height=8}
predictors <- time_items %>% 
  mutate(
    item_order = factor(display_order),
     times_item_shown = times_item_answered_factor,
     items_shown = factor(number_of_items_shown),
     last_item = relevel(factor(last_item), "none"),
     dnr = day_number,
     day_number = day_number_factor
     )

complex_mods <-  predictors %>% 
  split(.$item_name) %>%
  map(~ lmer(answer ~ items_shown + item_order + times_item_shown + day_number + refer_time_period +  (1 | session), data = .))


all_biases <- complex_mods %>% 
  map(~ tidy(., conf.int = TRUE, conf.level = 0.99)) %>% 
  bind_rows(.id = "response") 


all_biases %>% 
  filter(term != "refer_time_periodlast entry", 
         str_sub(term, 1, 10) != "day_number", term != "(Intercept)", !is.na(conf.high)) %>% 
  ggplot(aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  facet_wrap(~ response) +
  geom_linerange() +
  scale_y_continuous("95% CIs for regression coefficients") +
  coord_flip()
```

How much do our variables change when we residualise for all of these biases?

```{R}
predictors %>% 
  split(.$item_name) %>%
  map(~ cor(.$answer, residuals(lm(answer ~ items_shown + last_item + times_item_shown + day_number + refer_time_period, data = .)))) %>% 
  bind_rows() %>% 
  gather(variable, cor) %>% 
  arrange(cor) %>% 
  knitr::kable()
```

### Desire items

On a "very inaccurate" to "very accurate" response scale.

```{r}
desire_items %>% 
  filter(day_number < 7) %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(answer, na.rm = TRUE)) %>% 
  group_by(item_name, day_number, first_day_of_item_factor) %>% 
  filter(n_nonmissing(answer) > 20) %>% 
  ggplot(., aes(day_number, answer, colour = first_day_of_item_factor)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_hline(aes(yintercept = group_mean, group = label), color = "gray", linetype = 'dashed') +
  geom_pointrange(position = position_dodge(width = 0.2), stat = 'summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat = 'summary', fun.data = 'mean_se') + 
  scale_color_colorblind("First day the\nitem was shown") +
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, scales = "free_y", nrow = 3) + 
  scale_x_continuous("Day number", breaks = 0:10)
```

```{r}
desire_items %>% group_by(item_name, label_english, choices) %>% 
  summarise(women = n_distinct(session),
            days = n_distinct(unit_session_id)) %>% 
  knitr::kable()
```


```{r fig.width=10,fig.height=8}
predictors <- desire_items %>% 
  mutate(item_order = factor(display_order),
         times_item_shown = times_item_answered_factor,
         items_shown = factor(number_of_items_shown),
         last_item = relevel(factor(last_item), "none"),
         dnr = day_number,
         day_number = day_number_factor
         )

complex_mods <-  predictors %>% 
  split(.$item_name) %>%
  map(~ lmer(answer ~ items_shown + item_order + times_item_shown + day_number + refer_time_period +  (1 | session), data = .))


all_biases <- complex_mods %>% 
  map(~ tidy(., conf.int = TRUE, conf.level = 0.99)) %>% 
  bind_rows(.id = "response") 


all_biases %>% 
  filter(term != "refer_time_periodlast entry", 
         str_sub(term, 1, 10) != "day_number", term != "(Intercept)", !is.na(conf.high)) %>% 
  ggplot(aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  facet_wrap(~ response) +
  geom_linerange() +
  scale_y_continuous("95% CIs for regression coefficients") +
  coord_flip()
```

How much do our variables change when we residualise for all of these biases?

```{R}
predictors %>% 
  split(.$item_name) %>%
  map(~ cor(.$answer, residuals(lm(answer ~ items_shown + last_item + times_item_shown + day_number + refer_time_period, data = .)))) %>% 
  bind_rows() %>% 
  gather(item, cor) %>% 
  arrange(cor) %>% 
  knitr::kable()
```

### Retention items

On a "not at all" to "very much" response scale.

```{r}
retention_items %>% 
  filter(day_number < 7) %>% 
  group_by(item_name) %>% 
  mutate(group_mean = mean(answer, na.rm = TRUE)) %>% 
  group_by(item_name, day_number, first_day_of_item_factor) %>% 
  filter(n_nonmissing(answer) > 20) %>% 
  ggplot(., aes(day_number, answer, colour = first_day_of_item_factor)) + 
  geom_blank(aes(y = item_mean, ymin = item_mean - item_sd, ymax = item_mean + item_sd)) +
  geom_hline(aes(yintercept = group_mean, group = label), color = "gray", linetype = 'dashed') +
  geom_pointrange(position = position_dodge(width = 0.2), stat = 'summary', fun.data = 'mean_se') + 
  geom_line(position = position_dodge(width = 0.4), stat = 'summary', fun.data = 'mean_se') + 
  scale_color_colorblind("First day the\nitem was shown") +
  scale_y_continuous("Response") +
  facet_wrap(~ label_english, scales = "free_y", nrow = 3) + 
  scale_x_continuous("Day number", breaks = 0:10)
```

```{r}
retention_items %>% 
  group_by(item_name, label_english, choices) %>% 
  summarise(women = n_distinct(session),
            days = n_distinct(unit_session_id)) %>% 
  knitr::kable()
```




```{r fig.width=10,fig.height=8}
predictors <- retention_items %>% 
  mutate(
    item_order = factor(display_order),
    times_item_shown = times_item_answered_factor,
    items_shown = factor(number_of_items_shown),
    last_item = relevel(factor(last_item), "none"),
    dnr = day_number,
    day_number = day_number_factor
  )

complex_mods <-  predictors %>% 
  split(.$item_name) %>%
  map(~ lmer(answer ~ items_shown + item_order + times_item_shown + day_number + refer_time_period +  (1 | session), data = .))


all_biases <- complex_mods %>% 
  map(~ tidy(., conf.int = TRUE, conf.level = 0.99)) %>% 
  bind_rows(.id = "response")


all_biases %>% 
  filter(term != "refer_time_periodlast entry", 
         str_sub(term, 1, 10) != "day_number", term != "(Intercept)", !is.na(conf.high)) %>% 
  ggplot(aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  facet_wrap(~ response) +
  scale_y_continuous("95% CIs for regression coefficients") +
  geom_linerange() +
  coord_flip()
```

How much do our variables change when we residualise for all of these biases?

```{R}
predictors %>% 
  split(.$item_name) %>%
  map(~ cor(.$answer, residuals(lm(answer ~ items_shown + last_item + times_item_shown + day_number + refer_time_period, data = .)))) %>% 
  bind_rows() %>% 
  gather(item, cor) %>% 
  arrange(cor) %>% 
  knitr::kable()
```


## Dropout
```{r}

crosstabs(~ did_not_finish_entry + skipped_day, diary)

crosstabs(~ is.na(expired_diary) + is.na(ended_diary), diary)

ggplot(diary, aes(lag_nr_items_day, skipped_day)) + 
  geom_pointrange(stat='summary', fun.data='mean_se') +
  geom_smooth() +
  geom_smooth(method = 'lm') +
  xlim(20, 37)

ggplot(diary, aes(nr_items_day, did_not_finish_entry)) + 
  geom_pointrange(stat='summary', fun.data='mean_se') +
  geom_smooth() +
  geom_smooth(method = 'lm') +
  xlim(20, 37)

diary %>%  filter(!is.na(created_diary)) %>%  select(session, created_date, did_not_finish_entry, nr_items_day, lag_nr_items_day) %>% filter(did_not_finish_entry == 1) %>% arrange(session, created_date)

summary(lme4::lmer(skipped_day ~ lag_nr_items_day + nr_items_day + (1 + lag_nr_items_day + nr_items_day | session), diary %>% ungroup() %>% filter(between(nr_items_day, 20,37),
                                                                                                                                     between(lag_nr_items_day, 20,37))))
summary(m1 <- lme4::glmer(skipped_day ~ lag_nr_items_day + (1 + lag_nr_items_day | session), diary %>% ungroup() %>% filter(between(lag_nr_items_day, 20,37)) %>% 
                      mutate(lag_nr_items_day = lag_nr_items_day/10), family = binomial))
library(effects)
plot(allEffects(m1))

summary(lme4::glmer(skipped_day ~ lag_nr_items_day + (1 | session), diary %>% ungroup() %>% filter(between(lag_nr_items_day, 10,37)), family = binomial))
qplot(coef(m1)$session$lag_nr_items_day)
summary(glm(skipped_day ~ lag_nr_items_day, diary %>% ungroup() %>% filter(between(lag_nr_items_day, 10,37)), family = binomial))
```

## Shrout S1
```{r}
shrout1 <- rio::import("https://osf.io/ytz8f/download", format = "sas7bdat") %>% tbl_df()
shrout1
```

